%----------------------------------------------------------------------------------------
%	CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[10pt,a4paper,oneside]{article}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{lipsum}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[a4paper,left=2cm,right=2cm,top=2.5cm,bottom=2.5cm]{geometry}

%----------------------------------------------------------------------------------------
%	INFORMATION
%----------------------------------------------------------------------------------------

\title{Estudo de paralelismo no problema \textit{All-Pairs Shortest
    Paths}}

\author{Filipe Figueiredo\footnote{Filipe Figueiredo - 201203559},
  Pedro Paredes\footnote{Pedro Paredes - 201205725}, DCC - FCUP}

\date{Novembro 2015}

\renewcommand{\tablename}{Tabela}
\renewcommand{\figurename}{Figura}
\renewcommand{\refname}{Referências}
\newcommand{\BigO}[1]{\mathcal{O}(#1)}

\makeatletter
\renewcommand*{\ALG@name}{Algoritmo}
\makeatother

\begin{document}

\maketitle

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Introdução}
\label{sec:intro}
O problema do caminho mais curto (ou \textit{Shortest Path} em inglês)
é um problema clássico com grande aplicabilidade. O caminho mais curto
entre dois nós $u$ e $v$ consiste em determinar o caminho com vértice
inicial $u$ e vértice final $v$, tal que o custo do caminho é o menor
possível. O custo do caminho num grafo não pesado é determinado pelo
número de arestas que o caminho contém e num grafo pesado é
determinado pela soma dos pesos das arestas do caminho.

Historicamente o problema foi estudado usando duas estratégias
diferentes, para as quais existem vários algoritmos. A primeira,
chamada de \textit{Single-source shortest path} (ou caminho mais curto
a partir de um só vértice) tem como objetivo encontrar todos os
caminhos mais curtos que começam num determinado nó $u$. Já o
\textit{All-pairs shortest paths} (ou todos os pares de caminhos mais
curtos) tem como objetivo encontrar os caminhos mais curtos entre
todos os possíveis pares de nós.

Neste trabalho focamo-nos em usar a segunda estratégia em grafos
pesados. O objetivo será estudar o paralelismo existente em diferentes
algoritmos e obter uma implementação eficiente de modo a verificar a
sua escalabilidade.

O resto do relatório está organizado da seguinte forma. A
Secção~\ref{sec:ai} descreve em detalhe os algoritmos a estudar e as
suas versões paralelas. A Secção~\ref{sec:res} apresenta o estudo
empírico dos algoritmos através de testes com diferentes
redes. Finalmente, na Secção~\ref{sec:con} é feita uma breve conclusão
e apresentadas algumas notas finais.


%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Algoritmos e implementação}
\label{sec:ai}
Nesta secção estudaremos dois algoritmos conhecidos para o problema em
questão assim como uma possível paralelização para cada um. O primeiro
algoritmo é baseado numa variante de multiplicação de matrizes que é
aplicada várias vezes à matriz de adjacência do grafo. O segundo,
conhecido como o algoritmo de \textit{Floyd-Warshall}, usa a
subestrutura ótima do problema para percorrer e atualizar a matriz de
adjacência de forma eficiente. O segundo algoritmo tem uma
complexidade temporal inferior ao primeiro, mas a versão paralela do
primeiro permite escalar melhor a solução devido a necessitar de menos
comunicação entre processos.

\subsection{Algoritmo de multiplicação de matrizes sequencial}
O primeiro algoritmo a considerar é, como já indicado, um algoritmo
baseado numa variação da multiplicação de matrizes. Esta variação
passa por substituir as habituais operações de multiplicação e soma
presentes num algoritmo de multiplicação de matrizes comum por uma
operação de soma e minimização (escolher o mínimo entre o primeiro e o
segundo termo) respetivamente.

A correção deste algoritmo está ligada à sua ideia fundamental e pode
ser justificada argumentando que dado um caminho mais curto ótimo
entre dois vértices $u$ e $v$ (denotaremos o seu comprimento por
$d_{uv}$), seja $t$ um qualquer vértice nele contido, então $d_{uv} =
d_{ut} + d_{tv}$. Este facto é facilmente provado com um argumento de
maximalidade. Sendo assim, $d_{uv} = \min{\{d_{ut} + d_{tv}, t \in
  V\}}$, onde $V$ representa o conjunto de vértices do grafo. Esta
igualdade estabelece o paralelo da variação da multiplicação de
matrizes mencionada, pois dadas três matrizes $A, B, C$ tais que $A
\times B = C$, temos que $C_{uv} = \sum{A_{ut} \cdot B_{tv}}, t \in
       [1, n]$, onde $n$ representa a dimensão da matriz (assumimos
       que a matriz é quadrada para simplificar).

Para formalizar o algoritmo, denotaremos por $D^k$ a matriz de
distâncias entre todos os pares de pontos que usam no máximo $k$
arestas (onde $D^1$ é a matriz de adjacência do grafo, por
definição). É fácil de ver que $D^a \odot D^b = D^{a + b}$, onde
$\odot$ é a variação a multiplicação de matrizes vista anteriormente,
pois usando a igualdade estabelecida no parágrafo anterior, a soma de
todas as $D^a_{ut}$ com $D^b_{tv}$, para todo o $t$, representam a
junção de todos os caminhos com no máximo $a$ arestas com todos os
caminhos com no máximo $b$ arestas, e dada a igualdade definida, isto
corresponde a todos os caminhos com no máximo $a+b$ arestas.

Agora o objetivo do algoritmo é determinar $D^n$, sendo $n$ o número
de vértices (e mais uma unidade que o número máximo de arestas num
caminho mais curto), que pode ser obtido por: $D^n = D^1 \odot \ldots
\odot D^1$, com $n$ termos. Este valor pode ser obtido usando o
algoritmo de exponenciação por quadrados, que consiste em observar que
$D^{2m} = D^m \odot D^m$, e calcular $D^m$ apenas uma vez.

Os parágrafos anteriores estão sumarizados no Algoritmo~\ref{alg:alg1}.

A complexidade deste algoritmo é de $\BigO{|V|^3\log{(|V|)}}$, onde o
$|V|^3$ advém da multiplicação de matrizes e o fator logarítmico de
aplicar a exponenciação por quadrados. É de notar que apesar de
existirem algoritmos de multiplicação de matrizes de complexidade
inferior a cúbica, todos necessitam de operações de subtrair matrizes,
sendo aqui a subtração a operação inversa à soma, porém na variação
aplicada neste algoritmo não existe inverso da operação de minimização
e por isso esses algoritmos não se aplicam. De facto, o problema de
determinar se existe um algoritmo subcúbico para o problema de todos
os pares de caminhos mais curtos continua em aberto.

\begin{algorithm}[t]
\small
\caption{O algoritmo por multiplicação de matrizes repetida}
\renewcommand{\arraystretch}{0.85}
\textbf{Input:} A matriz de adjacência $A$ de um grafo\\
\textbf{Resultado:} A matriz de distâncias mais curtas $C$ de um grafo\\
\renewcommand{\arraystretch}{1.0}
\label{alg:alg1}
\begin{algorithmic}[1]
\State $C \leftarrow $ {\tt new Matrix} \Comment{$C$ inicialmente tem todas as entradas iguais a $\infty$}
\State $runs \leftarrow 1$
\While{$runs < |V|$}
  \ForAll{$u \in V$}
    \ForAll{$v \in V$}
      \ForAll{$t \in V$}
        \State $C_{uv} = \min{(C_{uv}, A_{ut} + A_{tv})}$
      \EndFor
    \EndFor
  \EndFor
  \State $runs \leftarrow runs \times 2$
\EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{Algoritmo de multiplicação de matrizes paralelo}
Iremos agora descrever uma possível paralelização deste algoritmo
baseada num algoritmo que paraleliza multiplicação de matrizes
conhecido por algoritmo de \textit{Fox}. Este algoritmo consiste em
aplicar uma decomposição da matriz de adjacência original $A$ em
blocos quadrados $B$ para serem atribuídos a um processo. Um exemplo
representativo desta decomposição esta esquematizado na
Tabela~\ref{tbl:tbl1}. Denotaremos as matrizes $B_{ij}$ por blocos e a
matriz de matrizes $B$ por matriz de blocos. Usaremos a notação $ij$
para denotar o processo que contém o bloco $B_{ij}$.

Com esta decomposição da matriz, a multiplicação da matriz é também
decomposta da seguinte forma: $B'_{ij} = \sum_k{B_{ik} \cdot B_{kj}}$,
ou na variação apresentada na subsecção anterior: $B'_{ij} =
\min_k{(B_{ik} \odot B_{kj})}$, onde aqui as operações são definidas para
matrizes (a minimização é elemento a elemento) e $B' = B^2$
(corresponde à matriz resultado).

Para que a decomposição possa ser efetuada, o número de processos tem
de ser um quadrado perfeito (para que possam ser distribuídos por uma
matriz de blocos como mostra a Tabela~\ref{tbl:tbl1}). Denotamos o
número de processos por $N_p$, definimos $q$ por $q = \sqrt{N_p}$ e
assumimos que $N_p$ é um quadrado perfeito, então cada bloco $B_{ij}$
terá dimensões de $\frac{|V|}{q}$ por $\frac{|V|}{q}$. Para que as
dimensões sejam inteiras é necessário que $|V|$ seja múltiplo de $q$,
logo na implementação deste algoritmo é feito um \textit{padding} de
$\infty$ na matriz de adjacência original de modo a que as suas
dimensões respeitem esta restrição.

Para efetuar o cálculo, a multiplicação a efetuar é coordenada em cada
linha de blocos da matriz, ou seja, todos os processos $ij$ de uma
determinada linha $i$ estarão a efetuar a multiplicação do mesma bloco
$B_{ik}$ com a matriz correspondente da sua coluna. Assim, o algoritmo
define que inicialmente todos os blocos irão efetuar a multiplicação
com a matriz do bloco da mesma linha que está contido na diagonal da
matriz de blocos. De seguida cada processo passa o seu bloco para o
processo imediatamente a cima na matriz de blocos (os blocos da
primeira linha passam para os da última linha) e recebe o bloco do
processo imediatamente a baixo. Este bloco recebido é denotado por
bloco extra e na iteração seguinte será esse bloco extra a ser enviado
para o processo imediatamente a cima (ou seja, na iteração inicial o
bloco extra é o próprio bloco do processo). O bloco extra será agora
usado na próxima multiplicação, que será feita com o bloco da mesma
linha que está na posição imediatamente à direita da diagonal (no caso
do bloco da diagonal estar na última coluna, então usar-se-á o da
primeira coluna). Este processo é repetido até que todas as
multiplicações necessárias sejam efetuadas, ou seja, que seja dada uma
volta completa para cada processo na matriz de blocos.

O método descrito no parágrafo anterior, pelo facto de coordenar as
multiplicações de blocos, diminui o número de comunicações que são
precisas de serem efetuadas e permite que sejam feitos {\tt
  broadcasts} por linha da matriz de blocos em cada iteração. Para
facilitar estas operações, na implementação em {\tt MPI} são definidos
um conjunto de comunicadores além do comunicador global: um
comunicador por linha que aglomera todos os processos em cada
linha da matriz de blocos, que permite que seja efetuado o {\tt
  broadcast} em cada iteração; um comunicador por coluna que aglomera
todos os processos numa coluna da matriz de blocos, que permite
que seja feita a passagem do bloco extra para o processo
imediatamente a cima (sendo esta troca mediada por um {\tt sendrecv}).

Os parágrafos anteriores são sumarizados no Algoritmo~\ref{alg:alg2}.

A complexidade deste algoritmo é $\BigO{\frac{\log{(|V|)} (|V|^3 +
    C_c|V|^2(1 + \log{(N_p)}))}{N_p}}$, onde $C_c$ é uma constante que
representa o custo de uma comunicação unitária (envio/receção de um
inteiro). O fator logarítmico advém da exponenciação quadrada, o fator
de $\frac{|V|^3}{N_p}$ advém de se fazerem $q$ multiplicações de
matrizes de dimensão $\frac{|V|}{q}$, o fator $\frac{C_c|V|^2}{N_p}$
advém de se fazerem $q$ {\tt sendrecv} de matrizes de dimensão
$\frac{|V|}{q}$ e finalmente o fator de
$\frac{C_c|V|^2\log{(N_p)}}{N_p}$ é análogo ao anterior mas onde é
feito um {\tt broadcast}.

\begin{table}[t]
  \centering
  \caption{Esquema de partição de uma matriz de adjacência $A$ de $6 \times 6$}
  \label{tbl:tbl1}
  \begin{tabular}{|c|c|c|}
    \hline
    Processo 1 & Processo 2 & Processo 3 \\
    $B_{1,1} = 
    \begin{pmatrix}
      A_{1,1} & A_{1,2} \\
      A_{2,1} & A_{2,2}
    \end{pmatrix}$ &
    $B_{1,2} = 
    \begin{pmatrix}
      A_{1,3} & A_{1,4} \\
      A_{2,3} & A_{2,4}
    \end{pmatrix}$ &
    $B_{1,3} = 
    \begin{pmatrix}
      A_{1,5} & A_{1,6} \\
      A_{2,5} & A_{2,6}
    \end{pmatrix}$ \\
    \hline
    Processo 4 & Processo 5 & Processo 6 \\
    $B_{2,1} = 
    \begin{pmatrix}
      A_{3,1} & A_{3,2} \\
      A_{4,1} & A_{4,2}
    \end{pmatrix}$ &
    $B_{2,2} = 
    \begin{pmatrix}
      A_{3,3} & A_{3,4} \\
      A_{4,3} & A_{4,4}
    \end{pmatrix}$ &
    $B_{2,3} = 
    \begin{pmatrix}
      A_{3,5} & A_{3,6} \\
      A_{4,5} & A_{4,6}
    \end{pmatrix}$ \\
    \hline
    Processo 7 & Processo 8 & Processo 9 \\
    $B_{3,1} = 
    \begin{pmatrix}
      A_{5,1} & A_{5,2} \\
      A_{6,1} & A_{6,2}
    \end{pmatrix}$ &
    $B_{3,2} = 
    \begin{pmatrix}
      A_{5,3} & A_{5,4} \\
      A_{6,3} & A_{6,4}
    \end{pmatrix}$ &
    $B_{3,3} = 
    \begin{pmatrix}
      A_{5,5} & A_{5,6} \\
      A_{6,5} & A_{6,6}
    \end{pmatrix}$ \\
    \hline
  \end{tabular}
\end{table}

\begin{algorithm}[b]
\small
\caption{O algoritmo paralelo por multiplicação de matrizes repetida}
\renewcommand{\arraystretch}{0.85}
\textbf{Input:} O bloco $B$ do processo atual\\
\textbf{Resultado:} A matriz de distâncias mais curtas $B'$ do bloco atual\\
\renewcommand{\arraystretch}{1.0}
\label{alg:alg2}
\begin{algorithmic}[1]
\State $B' \leftarrow $ {\tt new Matrix} \Comment{$B'$ inicialmente tem todas as entradas iguais a $\infty$}
\State $B^e \leftarrow B_{rc}$ \Comment{$r$ e $c$ são as linha e coluna do processo atual na matriz de blocos}
\State $runs \leftarrow 1$
\While{$runs < |V|$}
  \For{$i \in [1, q]$}
    \State $u \leftarrow (r + i) \mod q$
    \State $B^t \leftarrow$ {\tt broadcast} $(B_{ru}, r)$
    \State $B' = \min(B', B^e \odot B^t)$
    \State {\tt sendrecv} $(B^e, (c + 1) \mod q, (c - 1) \mod q)$
  \EndFor
  \State $runs \leftarrow runs \times 2$
\EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{Algoritmo de \textit{Floyd-Warshall} sequencial}
O algoritmo de \textit{Floyd-Warshall} usa programação dinâmica para
aproveitar resultados de maneira a diminuir o número de operações a
fazer. A ideia é, usando as observações indicadas nas subsecções
anteriores, de percorrer a matriz de adjacência da melhor forma.

Denotamos $D'^k$ como a matriz de distâncias mais curtas que apenas
considera caminhos com nós intermédios (ou seja, excluindo os
extremos) no conjunto $[1, k]$. O objetivo será calcular $D'^n$ (sendo
$n = |V|$) e temos que $D'^0$ é definido pela matriz de
adjacência. Sabendo $D'^{k - 1}$ é possível determinar $D'^k$
considerando $k$ como nó intermédio para todos os caminhos, ou seja,
$D'^k_{ij} = \min{(D'^{k - 1}_{ij}, D'^{k - 1}_{ik} + D'^{k -
    1}_{kj})}$. Sendo assim, é possível calcular $D'^n$ aplicando este
método $n$ vezes, considerando todos os vértices possíveis como $k$.

Os parágrafos anteriores estão sumarizados no Algoritmo~\ref{alg:alg3}.

A complexidade deste algoritmo é $\BigO{|V|^3}$, pois para cada
vértice é preciso percorrer a matriz de distâncias.

\begin{algorithm}[t]
\small
\caption{O algoritmo de \textit{Floyd-Warshall}}
\renewcommand{\arraystretch}{0.85}
\textbf{Input:} A matriz de adjacência $A$ de um grafo\\
\textbf{Resultado:} A matriz de distâncias mais curtas $C$ de um grafo\\
\renewcommand{\arraystretch}{1.0}
\label{alg:alg3}
\begin{algorithmic}[1]
\State $C \leftarrow A$
\ForAll{$k \in V$}
  \ForAll{$i \in V$}
    \ForAll{$j \in V$}
      \State $C_{ij} = \min{(C_{ij}, C_{ik} + C_{kj})}$
    \EndFor
  \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Algoritmo de \textit{Floyd-Warshall} paralelo}
Descrevemos agora uma possível paralelização do algoritmo sequencial
descrito na subsecção anterior. O objetivo é estaticamente dividir a
matriz de adjacência em ``tiras'' de $\frac{|V|}{N_p}$ linhas da
matriz (se $|V|$ não for divisível, alguns processadores terão menos
uma linha). Um exemplo representativo desta divisão está ilustrado na
Tabela~\ref{tbl:tbl2}.

Dada esta divisão, o algoritmo paralelo consiste em cada processo
calcular o resultado para a sua tira. Olhando para o algoritmo
sequencial, para cada iteração é necessário fazer uma operação do
tipo: $C_{ij} = \min{(C_{ij}, C_{ik} + C_{kj})}$. Cada processo contém
o valor de $C_{ij}$ (pois só calcula valores que estejam na sua tira)
e o valor de $C_{ik}$, pois pertence à mesma linha do valor a
calcular, logo o único valor que é potencialmente desconhecido é
$C_{kj}$. Sendo assim, em cada iteração é feito um {\tt broadcast} da
linha $k$ (que estará contida na tira de um dos processos) e será
usada para executar a operação acima descrita.

Os parágrafos anteriores estão sumarizados no Algoritmo~\ref{alg:alg3}.

A complexidade deste algoritmo é $\BigO{\frac{|V|^3 + C_c |V|^2
    \log{(N_p)}}{N_p}}$, onde o fator $\frac{|V|^3}{N_p}$ advém do
cálculo de cada tira, e o fator $\frac{C_c |V|^2 \log{(N_p)}}{N_p}$ vem
do {\tt broadcast} efetuado $|V|$ vezes de uma linha com $|V|$ valores
por todos os processos.

\begin{table}[t]
  \centering
  \caption{Esquema de partição de uma matriz de adjacência $A$ de $6 \times 6$}
  \label{tbl:tbl2}
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \multirow{2}{*}{Processo 1} & $A_{11}$ & $A_{12}$ & $A_{13}$ & $A_{14}$ & $A_{15}$ & $A_{16}$ \\ \cline{2-7}
    & $A_{21}$ & $A_{22}$ & $A_{23}$ & $A_{24}$ & $A_{25}$ & $A_{26}$ \\ \hline \hline
    \multirow{2}{*}{Processo 2} & $A_{31}$ & $A_{32}$ & $A_{33}$ & $A_{34}$ & $A_{35}$ & $A_{36}$ \\ \cline{2-7}
    & $A_{41}$ & $A_{42}$ & $A_{43}$ & $A_{44}$ & $A_{45}$ & $A_{46}$ \\ \hline \hline
    \multirow{2}{*}{Processo 3} & $A_{51}$ & $A_{52}$ & $A_{53}$ & $A_{54}$ & $A_{55}$ & $A_{56}$ \\ \cline{2-7}
    & $A_{61}$ & $A_{62}$ & $A_{63}$ & $A_{64}$ & $A_{65}$ & $A_{66}$ \\ \hline
  \end{tabular}
\end{table}

\begin{algorithm}[t]
\small
\caption{O algoritmo de \textit{Floyd-Warshall} paralelo}
\renewcommand{\arraystretch}{0.85}
\textbf{Input:} A tira $A'$ da matriz de adjacência $A$ de um grafo do processo atual\\
\textbf{Resultado:} A tira $C$ da matriz de distâncias mais curtas de um grafo\\
\renewcommand{\arraystretch}{1.0}
\label{alg:alg3}
\begin{algorithmic}[1]
\State $C \leftarrow A'$
\ForAll{$k \in V$}
 \State $C^k \leftarrow $ {\tt broadcast} $(C_k)$
  \ForAll{$i \in V'$}
    \ForAll{$j \in V$}
      \State $C_{ij} = \min{(C_{ij}, C_{ik} + C^k_{j})}$
    \EndFor
  \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------

\section{Resultados e discussão}
\label{sec:res}
Para testar a viabilidade e escalabilidade dos algoritmos discutidos
na Secção~\ref{sec:ai}, foram efetuados uma série de testes práticos
com \textit{inputs} diferentes. Os grafos usados como \textit{input}
estão descritos na Tabela~\ref{tbl:tbl3}.

Foram corridos os vários algoritmos com 1, 4, 9, 16 e 25 processos nas
máquinas do laboratório 1, tendo sido desligada a opção de ligação por
{\tt ssh} em árvore por razões técnicas. Os resultados obtidos estão
descritos na Tabela~\ref{tbl:tbl4}.

\begin{table}[t]
  \small
  \caption{Grafos usados nos testes}
  \label{tbl:tbl3}
  \centering
  \begin{tabular}{|c|c|l|}
    \hline
    Grafo & $|V|$ & Tipo/Fonte \\ \hline \hline
    {\tt Small}  &  300  & Dado no enunciado \\ \hline
    {\tt Medium} &  900  & Gerada por um modelo $ER$ \\ \hline
    {\tt Big}    &  1574 & Rede de voos \\ \hline
  \end{tabular}
\end{table}

\begin{table}[t]
  \caption{}
  \begin{tabular}{|l|r|r|r|r|r|r|r|r|r|r|}
    \hline
    & Sequencial & 1 & \multicolumn{2}{c|}{4} & \multicolumn{2}{c|}{9} & \multicolumn{2}{c|}{16} & \multicolumn{2}{c|}{25} \\ \hline
    & Tempo & Tempo & Tempo & Speedup & Tempo & Speedup & Tempo & Speedup & Tempo & Speedup \\ \hline
    {\tt Small} & 0.94 & 1.14 & 0.40 & 2.82 & 0.34 & 3.35 & 0.52 & 2.18 & 0.47 & 2.45 \\ \hline
    {\tt Medium} & 16.10 & 16.52 & 6.64 & 2.49 & 4.35 & 3.80 & 5.42 & 3.05 & 4.66 & 3.55 \\ \hline
    {\tt Big} & 487.71 & 518.18 & 71.79 & 7.22 & 29.89 & 17.34 & 25.96 & 19.96 & 20.33 & 25.49 \\ \hline
  \end{tabular}
  \label{}
\end{table}

%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------

\section{Conclusão e notas finais}
\label{sec:con}

\cite{shimbel1953structural}

\bibliographystyle{plain}
\bibliography{report}

\end{document}
